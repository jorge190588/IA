{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Funcion softmax de entropia cruzada con logits.\n",
    "\n",
    "# Definicion\n",
    "\n",
    "El nombre de la funcion en tensorflow es tf.nn.softmax_cross_entropy_with_logits.     El proposito de esta funcion medir el error de probabilidad en las tareas de clasificacion discretas en el cual las clases son mutuamente excluyentes, es decir que una entidad tiene una sola clase, como por ejemplo la entidad perro pertenece a la clase perro, una entidad no puede ser perro y gato al mismo tiempo, no puede ser representada por  una clase con nombre perroGato, esto es incorrecto.   En el paquete de imagenes CIFAR-10, cada imagen esta etiquetada con solo una etiqueta o un solo nombre, es decir que una imagen solo puede ser de una clase, como por ejemplo perro o gato, pero no ambas.  Mientras las clases son mutuamente excluyentes las probabilidades no lo son.   Los valores de las etiquetas pueden estar representados como valores resultantes de la funcion softmax o como valores one-hot encode.\n",
    "\n",
    "Una restriccion es que el parametro logits y etiquetas (labels) deben tener la misma forma y el mismo tipo.   La forma de logits y las etiquetas (labels) puede ser de la siguiente forma [tamano_Lote, numero_clasess] y el tipo puede ser float16, float32 o float64.\n",
    "\n",
    "# Sintaxis\n",
    "\n",
    "La funcion en tensorFlow debe ser ejecutada de la siguiente forma:\n",
    "\n",
    "    - softmax_cross_entropy_with_logits(_sentinel=None,labels=None,logits=None,dim=-1,name=None)\n",
    "\n",
    "Los parametro de la funcion son los siguientes:\n",
    "    - _sentinel = usado para prevenir los parametros posicionales.  Uso interno, no usar.\n",
    "    - labels = Cada fila representada por etiqueta[indice] (label[i]), debe ser una distribucion de probabilidad valida.\n",
    "    - logits = registro de las probabilidades sin escalar en un rango [-infinito, +infinito]\n",
    "    - dim = La clase de dimensiones.   El valor predeterminado es -1, el cual significa que es la ultima clase.\n",
    "    - name= Representa un nombre para la operacion.  Valor opcional.\n",
    "\n",
    "\n",
    "La suma de las entradas para esta funcion puede cualquier numero, mayor o menor a cero, porque los valores no son probabilidades.\n",
    "\n",
    "Para conocer mas sobre el parametro logits consulte el archivo con nombre logits.ipynb\n",
    "\n",
    "# Objectivo\n",
    "\n",
    "Medir el error de las probabilidades por cada clase en la capa totalmente conectada o fully connected layer.\n",
    "\n",
    "Fitura 1. Capas de una red neuronal.\n",
    "![](https://adeshpande3.github.io/assets/Cover.png)\n",
    "\n",
    "\n",
    "# Ejemplos:\n",
    " * Ejemplo 1: calcula la activacion softmax con etropia de dos tensores, etiquetas (EtiquetasVerdaderasOneHot) y valores (valoresDelTensor).\n",
    " * Ejemplo 2: Demostrar el procedimiento corto y largo para calcular el total de error de la entropia cruzada, en el procedimiento corto se muestra el calculo de la entropia con aplicacion la formula tf.nn.softmax, tf.log, tf.reduce_sum y tf.reduce_mean\n",
    " * Ejemplo 3: Ejemplo 3: Demostrar la diferencia de las probabilidades de distribucion entre la aplicacion de la funcion softmax y la funcion softmax con entropia cruzada\n",
    "\n",
    "# Referencias\n",
    "* [Stackoverflow.com (2,017). What's the difference between softmax and softmax_cross entropy with logits?](https://stackoverflow.com/questions/34240703/whats-the-difference-between-softmax-and-softmax-cross-entropy-with-logits)\n",
    "* [What is the meaning of the word logits in tensorflow](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow)\n",
    "* [tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "* [Calculating cross entropy in tensorflow](https://stackoverflow.com/questions/42521400/calculating-cross-entropy-in-tensorflow)\n",
    "* [tf.nn.softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)\n",
    "* [How to choose cross-entropy loss in tensorflow ?](https://stackoverflow.com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensorflow/47034889)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perdida total en la entropiza cruzada \n",
      "[ 0.16219762  1.44253553  1.15464509  1.71115412]\n",
      "\n",
      "Equiquetas verdaderas one hot \n",
      "[[ 0.1  0.   0.   0. ]\n",
      " [ 0.   1.   0.   0. ]\n",
      " [ 0.   0.   1.   0. ]\n",
      " [ 0.   0.   0.   1. ]]\n",
      "\n",
      "softmax con cross entropy : \n",
      " [ 0.16219762  1.44253553  1.15464509  1.71115412]\n",
      "\n",
      "Error de probabilidad en la clasificacion : \n",
      " 1.11763309191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EJEMPLO 1: Calcular el error de la probabilidad en la tareas de clasificacion.\n",
    "# IMPORTS\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# CREATE SESSION\n",
    "session = tf.Session()\n",
    "\n",
    "#CREATE VARIABLES AND PLACEHOLDERS\n",
    "valoresDelTensor = tf.constant(np.array([[-.2, .0, .1, .2],[.3, 0.4, .5, .6],[.7, .8, .9 , .1],[.7, .3, .5 , .1]]))\n",
    "EtiquetasVerdaderasOneHot = tf.convert_to_tensor(np.array([[0.1, 0.0, 0.0, 0.0],[0.0, 1.0, 0.0, 0.0],[0.0, 0.0, 1.0, 0.0],[0.0, 0.0, 0.0, 1.0]]))\n",
    "\n",
    "#INIT VARIABLES\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "#RUN SESSION\n",
    "EtiquetasVerdaderasOneHotResultado = session.run(EtiquetasVerdaderasOneHot)\n",
    "softmaxWitCrossEntropy= tf.nn.softmax_cross_entropy_with_logits(None,EtiquetasVerdaderasOneHotResultado,valoresDelTensor,name=\"softmax\")\n",
    "softmaxWitCrossEntropyResult= session.run(softmaxWitCrossEntropy)\n",
    "\n",
    "#Reduce_mean calcula la media a travez de las dimensiones de un tensor.\n",
    "media=tf.reduce_mean(softmaxWitCrossEntropyResult)\n",
    "mediaResultado=session.run(media)\n",
    "\n",
    "print(\"Equiquetas verdaderas one hot \\n\"+str(EtiquetasVerdaderasOneHotResultado)+\"\\n\")\n",
    "print(\"Resutlado de la funcion softmax con cross entropy : \\n \"+str(softmaxWitCrossEntropyResult)+\"\\n\")\n",
    "print(\"Media de Error de probabilidad en la clasificacion : \\n \"+str(mediaResultado)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ejemplo 2\n",
    "\n",
    "Demostrar el procedimiento corto y largo para calcular el total de error de la entropia cruzada, en el procedimiento corto se muestra el calculo de la entropia con aplicacion la formula tf.nn.softmax, tf.log, tf.reduce_sum y tf.reduce_mean\n",
    "\n",
    "# Funciones utilizadas\n",
    "\n",
    "* tf.nn.softmax: convierter un tensor con valones no normalizados en el rango [-infinito, +infinito] a un tensor normalizado con valores en el rango [0,1].\n",
    "* tf.log: calcula el logaritmo de un tensor.\n",
    "* tf.reduce_sum: calcula la suma de las dimensiones de un tensor\n",
    "* tf.reduce_mean: calcula la media de la dimensiones de un tensor\n",
    "\n",
    "# Referencias\n",
    "* [tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones calculadas por clase \n",
      "[[ 0.5  1.5  0.1]\n",
      " [ 2.2  1.3  1.7]]\n",
      "\n",
      "Equiquetas verdades one hot \n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "\n",
      "Probabilidades normalizadas por cada clase usando softmax \n",
      "[[ 0.227863    0.61939586  0.15274114]\n",
      " [ 0.49674623  0.20196195  0.30129182]]\n",
      "\n",
      "Perdida total (Proc. largo) \n",
      "3.65211878273\n",
      "Perdida total (Proc. corto) \n",
      "3.65211878273\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 2.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "sess = tf.Session()\n",
    "\n",
    "# Procedimiento corto:\n",
    "\n",
    "# PuntuacionesCalculadasPorClase: contiene los punteos calculados para cada clases por ejemplo: W*x +b\n",
    "# los resultados pueden ser interpretados como las etiquetas predichas para las clases [a, b y c]\n",
    "arregloDeDatos=np.array([[0.5, 1.5, 0.1],[2.2, 1.3, 1.7]])\n",
    "PuntuacionesCalculadasPorClase = tf.convert_to_tensor(arregloDeDatos)\n",
    "PuntuacionesCalculadasPorClaseResult=sess.run(PuntuacionesCalculadasPorClase)\n",
    "print(\"Puntuaciones calculadas por clase \\n\"+str(PuntuacionesCalculadasPorClaseResult)+\"\\n\")\n",
    "\n",
    "EtiquetasVerdaderasOneHot = tf.convert_to_tensor(np.array([[0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]))\n",
    "EtiquetasVerdaderasOneHotResultado = sess.run(EtiquetasVerdaderasOneHot)\n",
    "print(\"Equiquetas verdades one hot \\n\"+str(EtiquetasVerdaderasOneHotResultado)+\"\\n\")\n",
    "\n",
    "# Calcular la perdiad total de entropia cruzada.\n",
    "# Forma 1: Procedimiento largo, calcular la perdida total de la entropia cruzada. \n",
    "\n",
    "PuntuacionesCalculadasPorClaseSoftmax = tf.nn.softmax(PuntuacionesCalculadasPorClase)\n",
    "PuntuacionesCalculadasPorClaseSoftmaxResult = sess.run(PuntuacionesCalculadasPorClaseSoftmax)\n",
    "print(\"Probabilidades normalizadas por cada clase usando softmax \\n\"+str(PuntuacionesCalculadasPorClaseSoftmaxResult)+\"\\n\")\n",
    "\n",
    "perdidaTotal = tf.reduce_mean(-tf.reduce_sum(PuntuacionesCalculadasPorClase * tf.log(PuntuacionesCalculadasPorClaseSoftmax),[1]))\n",
    "perdidaTotalResultado = sess.run(perdidaTotal)\n",
    "print(\"Perdida total (Proc. largo) \\n\"+str(perdidaTotalResultado))\n",
    "\n",
    "#Forma 2: Procedimiento corto, es el equivalente del procedimiento corto para calcular la perdida total de entropia cruzada.\n",
    "perdidaTotal2 = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        _sentinel=None,\n",
    "                        logits=PuntuacionesCalculadasPorClaseSoftmaxResult,\n",
    "                        labels=EtiquetasVerdaderasOneHotResultado))\n",
    "\n",
    "perdidaTotal2Resultado = sess.run(perdidaTotal2)\n",
    "print(\"Perdida total (Proc. corto) \\n\"+str(perdidaTotalResultado))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ejemplo 3\n",
    "\n",
    "La capa de salida o \"Fully connected layer\" de una red neuronal se debe calcular un arreglo que contiene los punteos de las clases para cada una de las instancias de entrenamiento, estas instancias son generadas a partir del calculo salida_de_sombrero = W*x + b.  Si salida_de_sombrero es una arreglo de 2x3 dimensiones, el numero de filas es 2 y el numero de columnas es 3, las filas son las instancias y las columnas son las clases de sombreros, entonces el arreglo tiene 2 instancias y 3 clases.\n",
    "\n",
    "Tabla 1. Representacion de clases e instancias de un arreglo.\n",
    "\n",
    "| Instancias  | Clase A     | Clase B     | Clase C     |\n",
    "| ------      | ------      | ------      | ------      |\n",
    "| Instancia 1 | 0.227863    | 0.61939586  | 0.15274114  |\n",
    "| Instancia 2 | 0.49674623  | 0.20196195  | 0.30129182  |\n",
    "\n",
    "\n",
    "Fitura 1. Capas de una red neuronal.\n",
    "![](https://adeshpande3.github.io/assets/Cover.png)\n",
    "\n",
    "# Descripcion de los pasos.\n",
    "* Paso 1. Datos de entrada, son valores por clase no normalizados.\n",
    "* Paso 2. Valores por clase normalizados con la funcion softmax.\n",
    "* Paso 3. Resultados de probabilidad para cada clases de todas las instancias de entrenamiento.\n",
    "* Paso 4. Modificar las puntuaciones por clase\n",
    "* Paso 5, calcular la perdida por instancia, forma 1, se calcula el logaritmo sobre la puntuaciones calculadas con la funcion softmax.\n",
    "* Paso 6, calcular la perdida total sin usar la funcion softmax con entropia cruzada, si se usan los valores resultantes de la funcion softmax y se aplica la funcion log, reduce_sum y reduce_mean\n",
    "* Paso 7, calcular la perdida por instancia, forma 2, se calcula la perdida por instancia de los valores originales sin haber aplicado la funcion softmax, es la diferencia del paso 5 y el paso 7.\n",
    "* Paso 8, calcular la perdidad total usando la funcion softmax con entropia cruzada, se usan los valores sin haber aplicado la funcion softmax.\n",
    "\n",
    "# Conclusion:\n",
    "\n",
    "Existe una diferencia minima en los decimales entre la perdidaTotalForma1 y perdidaTotalForma2, la perdidaTotalForma1 es mas extensa porque a los valores originales se le aplica la funcion softmax con el objetivo de normalizar los valores, para luego aplicarle la funcion reduce_sum y recude_min.  En la perdidaTotalForma2 no se aplica la funcion softmax, debido a que el tensor de entrada no requiere que los valores esten normalizados.\n",
    "\n",
    "# Funciones utilizadas\n",
    "\n",
    "* tf.nn.softmax: convierter un tensor con valones no normalizados en el rango [-infinito, +infinito] a un tensor normalizado con valores en el rango [0,1].\n",
    "* tf.log: calcula el logaritmo de un tensor.\n",
    "* tf.reduce_sum: calcula la suma de las dimensiones de un tensor\n",
    "* tf.reduce_mean: calcula la media de la dimensiones de un tensor\n",
    "\n",
    "# Referencias\n",
    "* [tf.nn.softmax_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puntuaciones calculadas por clase (No normalizadas) \n",
      "[[ 0.5  1.5  0.1]\n",
      " [ 2.2  1.3  1.7]]\n",
      "\n",
      "Puntuaciones calculadas por clase (Normalizadas con softmax) \n",
      "[[ 0.227863    0.61939586  0.15274114]\n",
      " [ 0.49674623  0.20196195  0.30129182]]\n",
      "\n",
      "Arreglo de puntuacions por clase \n",
      "[[ 0.227863    0.61939586  0.15274114]\n",
      " [ 0.49674623  0.20196195  0.30129182]]\n",
      "\n",
      "Probabilidades por instancia \n",
      "[1 0 1]\n",
      "\n",
      "Puntuaciones calculadas por clase (No normalizadas) \n",
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "\n",
      "Perdida por instancia (forma 1) \n",
      "[ 0.4790107   1.19967598]\n",
      "\n",
      "Perdida total (sin funcion de entropia cruzada) \n",
      "0.839343338979\n",
      "\n",
      "Perdida por instancia (forma 2) \n",
      "[ 0.4790107   1.19967598]\n",
      "\n",
      "Perdida total (con funcion de entropia cruzada) \n",
      "0.839343338979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 3: Demostrar la diferencia de las probabilidades de distribucion entre la aplicacion de la funcion softmax y la funcion softmax con entropia cruzada\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "sess = tf.Session()\n",
    "\n",
    "# Paso 1. Valores por clase no normalizados.\n",
    "# PuntuacionesCalculadasPorClase: contiene los punteos calculados para cada clases por ejemplo: W*x +b\n",
    "# los resultados pueden ser interpretados como las etiquetas predichas para las clases [a, b y c]\n",
    "# Los resultados no estan normalizado, los valores estan en el rango [-infinito, +infinito]\n",
    "arregloDeDatos=np.array([[0.5, 1.5, 0.1],[2.2, 1.3, 1.7]])\n",
    "PuntuacionesCalculadasPorClase = tf.convert_to_tensor(arregloDeDatos)\n",
    "PuntuacionesCalculadasPorClaseResult=sess.run(PuntuacionesCalculadasPorClase)\n",
    "print(\"Puntuaciones calculadas por clase (No normalizadas) \\n\"+str(PuntuacionesCalculadasPorClaseResult)+\"\\n\")\n",
    "\n",
    "# Paso 2. Valores por clase normalizado con la funcion softmax.\n",
    "# Es importante entender los resultados de la funcion softmax, la interpretancion de los resultados es la siguiente, la probabilidad del entrenammiento de la instancia 1 y la clase B es de 0.61639578, asi mismo se pueden analizar los resultados para cada instancia por las clases existentes.\n",
    "\n",
    "PuntuacionesCalculadasPorClaseSoftmax = tf.nn.softmax(PuntuacionesCalculadasPorClase)\n",
    "PuntuacionesCalculadasPorClaseSoftmaxResult = sess.run(PuntuacionesCalculadasPorClaseSoftmax)\n",
    "print(\"Puntuaciones calculadas por clase (Normalizadas con softmax) \\n\"+str(PuntuacionesCalculadasPorClaseSoftmaxResult)+\"\\n\")\n",
    "\n",
    "# Paso 3. Resultados de probabilidad para cada clases de todas las instancias de entrenamiento.\n",
    "# Al aplicar la funcion argmax de cada instancia se puede obtener la probabilidad para cada clase, en los resultados anteriores el valor mas alto de la instancia 1 es la clase B y para la instancia 2 el resultado es la clase C.\n",
    "\n",
    "ProbabilidadesPorInstancia = tf.argmax(PuntuacionesCalculadasPorClaseSoftmaxResult)\n",
    "ProbabilidadesPorInstanciaResultado = sess.run(ProbabilidadesPorInstancia)\n",
    "print(\"Probabilidades por instancia \\n\"+str(ProbabilidadesPorInstanciaResultado)+\"\\n\")\n",
    "\n",
    "# Paso 4. Modificar las puntuaciones por clase\n",
    "#La pregunta mas importante es como sabemos si la clasificacion es correcta.  Para responder esta pregunta es necesario comparar la probabilidad de las puntuaciones por clase \"PuntuacionesCalculadasPorClase\" con la probabilidad de las puntuaciones por clase de la funcion softmax.    Para medir el error de las puntuaciones calculadas por clase con respecto a los valores de la funcion softmax es necesario aplicar la funcion cross-entropy loss.\n",
    "\n",
    "PuntuacionesCalculadasPorClaseEjemplo2 = tf.convert_to_tensor(np.array([[0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]))\n",
    "PuntuacionesCalculadasPorClaseEjemplo2Resultado = sess.run(PuntuacionesCalculadasPorClaseEjemplo2)\n",
    "print(\"Puntuaciones calculadas por clase (No normalizadas) \\n\"+str(PuntuacionesCalculadasPorClaseEjemplo2Resultado)+\"\\n\")\n",
    "\n",
    "# Paso 5, calcular la perdida por instancia, forma 1, se calcula el logaritmo sobre la puntuaciones calculadas con la funcion softmax.\n",
    "perdidadDeInstanciaForma1 = -tf.reduce_sum(\n",
    "                                PuntuacionesCalculadasPorClaseEjemplo2 * tf.log(PuntuacionesCalculadasPorClaseSoftmax),\n",
    "                                reduction_indices=[1])\n",
    "\n",
    "perdidadDeInstanciaForma1Resultado = sess.run(perdidadDeInstanciaForma1)\n",
    "print(\"Perdida por instancia (forma 1) \\n\"+str(perdidadDeInstanciaForma1Resultado)+\"\\n\")\n",
    "\n",
    "# Paso 6, calcular la perdida total sin usar la funcion softmax con entropia cruzada, si se usan los valores resultantes de la funcion softmax y se aplica la funcion log, reduce_sum y reduce_mean\n",
    "perdidaTotal = tf.reduce_mean(\n",
    "                -tf.reduce_sum(\n",
    "                    PuntuacionesCalculadasPorClaseEjemplo2 * tf.log(PuntuacionesCalculadasPorClaseSoftmax),\n",
    "                    reduction_indices=[1]))\n",
    "\n",
    "perdidaTotalResultado= sess.run(perdidaTotal)\n",
    "print(\"Perdida total (sin funcion de entropia cruzada) \\n\"+str(perdidaTotalResultado)+\"\\n\")\n",
    "\n",
    "# Paso 7, calcular la perdida por instancia, forma 2, se calcula la perdida por instancia de los valores originales sin haber aplicado la funcion softmax, es la diferencia del paso 5 y el paso 7.\n",
    "perdidaPorInstanciaForma2 = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        _sentinel=None,\n",
    "                        logits=PuntuacionesCalculadasPorClase,\n",
    "                        labels=PuntuacionesCalculadasPorClaseEjemplo2)\n",
    "\n",
    "perdidaPorInstanciaForma2Resultado = sess.run(perdidaPorInstanciaForma2)\n",
    "print(\"Perdida por instancia (forma 2) \\n\"+str(perdidaPorInstanciaForma2Resultado)+\"\\n\")\n",
    "\n",
    "# Paso 8, calcular la perdidad total usando la funcion softmax con entropia cruzada, se usan los valores sin haber aplicado la funcion softmax.\n",
    "perdidaTotalForma2 = tf.reduce_mean(\n",
    "                        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                            _sentinel=None,\n",
    "                            logits=PuntuacionesCalculadasPorClase,\n",
    "                            labels=PuntuacionesCalculadasPorClaseEjemplo2))\n",
    "\n",
    "perdidaTotalForma2Resultado = sess.run(perdidaTotalForma2)\n",
    "print(\"Perdida total (con funcion de entropia cruzada) \\n\"+str(perdidaTotalForma2Resultado)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
